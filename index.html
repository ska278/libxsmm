<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>LIBXSMM by hfp</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">LIBXSMM</h1>
        <p class="header">Library targeting Intel Architecture (x86) for small, dense or sparse matrix multiplications, and small convolutions.</p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/hfp/libxsmm/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/hfp/libxsmm/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/hfp/libxsmm">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/hfp">hfp</a></p>


      </header>
      <section>
        <h1>
<a id="libxsmm" class="anchor" href="#libxsmm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/hfp/libxsmm/raw/master/documentation/libxsmm.pdf">LIBXSMM</a>
</h1>

<p><a href="LICENSE"><img src="https://img.shields.io/badge/license-BSD3-blue.svg" alt="License"></a> <a href="https://github.com/hfp/libxsmm/archive/master.zip"><img src="https://travis-ci.org/hfp/libxsmm.svg?branch=master" alt="Travis CI" title="Master branch build status"></a> <a href="https://buildkite.com/intel/intel-2017" title="Build status"><img src="https://badge.buildkite.com/63b5dc4095f460f1c011ae782f8e67ec0b8a6a9732d8abe3c7.svg" alt="Travis Mirror"></a></p>

<p>LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications as well as for deep learning primitives such as small convolutions targeting Intel Architecture (x86). The library is generating code for the following instruction set extensions: Intel SSE, Intel AVX, Intel AVX2, IMCI (KNCni) for Intel Xeon Phi coprocessors ("KNC"), and Intel AVX‑512 as found in the <a href="https://software.intel.com/en-us/articles/what-disclosures-has-intel-made-about-knights-landing">Intel Xeon Phi processor family ("KNL")</a> and Intel Xeon processors (Skylake-EP "SKX"). Small convolutions are currently only optimized for Intel AVX‑512. Historically the library was solely targeting the Intel Many Integrated Core Architecture "MIC") using intrinsic functions, meanwhile optimized assembly code is targeting all aforementioned instruction set extensions (static code generation), and Just‑In‑Time (JIT) code generation is targeting Intel AVX and beyond.</p>

<p><strong>What is the background of the name "LIBXSMM"?</strong> The "MM" stands for Matrix Multiplication, and the "S" clarifies the working domain i.e., Small Matrix Multiplication. The latter also means the name is neither a variation of "MXM" nor an eXtreme Small Matrix Multiplication but rather about Intel Architecture (x86) - and no, the library is <a href="https://github.com/hfp/libxsmm/issues/103#issuecomment-256887962">64‑bit only</a>. The spelling of the name might follow the syllables of libx\/smm, libx'smm, or libx‑smm.</p>

<p><strong>What is a small matrix multiplication?</strong> When characterizing the problem size using the M, N, and K parameters, a problem size suitable for LIBXSMM falls approximately within (M N K)<sup>1/3</sup> &lt;= 80 (which illustrates that non-square matrices or even "tall and skinny" shapes are covered as well). The library is typically used to generate code up to the specified <a href="#auto-dispatch">threshold</a>. Raising the threshold may not only generate excessive amounts of code (due to unrolling in M and K dimension), but also miss to implement a tiling scheme to effectively utilize the cache hierarchy. For auto-dispatched problem sizes above the configurable threshold, LIBXSMM is falling back to BLAS.</p>

<p><strong>What about "medium-sized" matrix multiplication?</strong> A more recent addition are GEMM routines which are parallelized using OpenMP (<code>libxsmm_?gemm_omp</code>). These routines leverage the same specialized kernel routines as the small matrix multiplications, in-memory code generation (JIT), and automatic code/parameter dispatch but they are implementing a tile-based multiplication scheme i.e., a scheme suitable for larger problem sizes.</p>

<p><strong>How to determine whether an application can benefit from using LIBXSMM or not?</strong> Given the application uses BLAS to carry out matrix multiplications, one may use the <a href="#call-wrapper">Call Wrapper</a>, and measure the application performance e.g., time to solution. However, the latter can significantly improve when using LIBXSMM's API directly. To check whether there are applicable GEMM-calls, the <a href="#verbose-mode">Verbose Mode</a> can help to collect an insight. Further, when an application uses <a href="https://registrationcenter.intel.com/en/forms/?productid=2558">Intel MKL 11.2</a> (or higher), then running the application with the environment variable MKL_VERBOSE=1 (<code>env MKL_VERBOSE=1 ./workload &gt; verbose.txt</code>) can collect a similar insight (<code>grep -a "MKL_VERBOSE DGEMM(N,N" verbose.txt | cut -d'(' -f2 | cut -d, -f3-5"</code>).</p>

<p><strong>What is a small convolution?</strong> In the last years, new workloads such as deep learning and more specifically convolutional neural networks (CNN) emerged, and are pushing the limits of today's hardware. One of the expensive kernels is a small convolution with certain kernel sizes (3, 5, or 7) such that calculations in the frequency space is not the most efficient method when compared with direct convolutions. LIBXSMM's current support for convolutions aims for an easy to use invocation of small (direct) convolutions, which are intended for CNN training and classification. The <a href="#interface-for-convolutions">Interface</a> is currently ramping up, and the functionality increases quickly towards a broader set of use cases.</p>

<h2>
<a id="interface-for-matrix-multiplication" class="anchor" href="#interface-for-matrix-multiplication" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interface for Matrix Multiplication</h2>

<p>The interface of the library is <em>generated</em> according to the <a href="#build-instructions">Build Instructions</a>, and it is therefore <strong>not</strong> stored in the code repository. Instead, one may have a look at the code generation template files for <a href="https://github.com/hfp/libxsmm/blob/master/src/template/libxsmm.h">C/C++</a> and <a href="https://github.com/hfp/libxsmm/blob/master/src/template/libxsmm.f">FORTRAN</a>.</p>

<p>In order to initialize the dispatch-table or other internal resources, an explicit initialization routine helps to avoid lazy initialization overhead when calling LIBXSMM for the first time. The library deallocates internal resources at program exit, but also provides a companion to the aforementioned initialization (finalize).</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** Initialize the library; pay for setup cost at a specific point. */</span>
<span class="pl-k">void</span> <span class="pl-en">libxsmm_init</span>(<span class="pl-k">void</span>);
<span class="pl-c">/** De-initialize the library and free internal memory (optional). */</span>
<span class="pl-k">void</span> <span class="pl-en">libxsmm_finalize</span>(<span class="pl-k">void</span>);</pre></div>

<p>To perform the dense matrix-matrix multiplication <em>C<sub>m x n</sub> = alpha · A<sub>m x k</sub> · B<sub>k x n</sub> + beta · C<sub>m x n</sub></em>, the full-blown GEMM interface can be treated with "default arguments" (which is deviating from the BLAS standard, however without compromising the binary compatibility).</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** Automatically dispatched dense matrix multiplication (single/double-precision, C code). */</span>
libxsmm_?gemm(<span class="pl-c1">NULL</span><span class="pl-c">/*transa*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*transb*/</span>, &amp;m<span class="pl-c">/*required*/</span>, &amp;n<span class="pl-c">/*required*/</span>, &amp;k<span class="pl-c">/*required*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*alpha*/</span>, a<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*lda*/</span>, b<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldb*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*beta*/</span>, c<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldc*/</span>);
<span class="pl-c">/** Automatically dispatched dense matrix multiplication (C++ code). */</span>
<span class="pl-en">libxsmm_gemm</span>(<span class="pl-c1">NULL</span><span class="pl-c">/*transa*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*transb*/</span>, m<span class="pl-c">/*required*/</span>, n<span class="pl-c">/*required*/</span>, k<span class="pl-c">/*required*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*alpha*/</span>, a<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*lda*/</span>, b<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldb*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*beta*/</span>, c<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldc*/</span>);</pre></div>

<p>For the C interface (with type prefix 's' or 'd'), all arguments and in particular m, n, and k are passed by pointer. This is needed for binary compatibility with the original GEMM/BLAS interface. The C++ interface is also supplying overloaded versions where m, n, and k are allowed to be passed by‑value (making it clearer that m, n, and k are non-optional arguments).</p>

<p>The FORTRAN interface supports optional arguments (without affecting the binary compatibility with the original BLAS interface) by allowing to omit arguments where the C/C++ interface allows for NULL to be passed.</p>

<div class="highlight highlight-source-fortran-modern"><pre><span class="pl-c">! Automatically dispatched dense matrix multiplication (single/double-precision).</span>
<span class="pl-k">CALL</span> libxsmm_?gemm(m<span class="pl-k">=</span>m, n<span class="pl-k">=</span>n, k<span class="pl-k">=</span>k, a<span class="pl-k">=</span>a, b<span class="pl-k">=</span>b, c<span class="pl-k">=</span>c)
<span class="pl-c">! Automatically dispatched dense matrix multiplication (generic interface).</span>
<span class="pl-k">CALL</span> libxsmm_gemm(m<span class="pl-k">=</span>m, n<span class="pl-k">=</span>n, k<span class="pl-k">=</span>k, a<span class="pl-k">=</span>a, b<span class="pl-k">=</span>b, c<span class="pl-k">=</span>c)</pre></div>

<p>For convenience, a BLAS-based dense matrix multiplication (<code>libxsmm_blas_gemm</code>) is provided for all supported languages which is simply re-exposing the underlying GEMM/BLAS implementation. The BLAS-based GEMM might be useful for validation/benchmark purposes, and more important as a fallback when building an application-specific dispatch mechanism.</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** Automatically dispatched dense matrix multiplication (single/double-precision). */</span>
libxsmm_blas_?gemm(<span class="pl-c1">NULL</span><span class="pl-c">/*transa*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*transb*/</span>, &amp;m<span class="pl-c">/*required*/</span>, &amp;n<span class="pl-c">/*required*/</span>, &amp;k<span class="pl-c">/*required*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*alpha*/</span>, a<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*lda*/</span>, b<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldb*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*beta*/</span>, c<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldc*/</span>);</pre></div>

<p>A more recently added variant of matrix multiplication is parallelized based on the OpenMP standard. The associated routines will open an internal parallel region by default, however participating on an already opened parallel region (without relying on nested parallelism) is also possible by using the environment variable LIBXSMM_MT (0: small-sized, 1: sequential, and 2: parallelized/default). The actual parallelism is based on "classic" OpenMP by default (thread-based), but can be adjusted to OpenMP 3.0 task-based parallelism (environment variable LIBXSMM_TASKS=1). At least the latter parallelization is dynamically scheduled. Please note that these routines are hosted by the extension library (libxsmmext) keeping the main library agnostic with respect to a particular threading runtime.</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** OpenMP parallelized dense matrix multiplication (single/double-precision). */</span>
libxsmm_?gemm_omp(&amp;transa, &amp;transb, &amp;m, &amp;n, &amp;k, &amp;alpha, a, &amp;lda, b, &amp;ldb, &amp;beta, c, &amp;ldc);</pre></div>

<p>Successively calling a particular kernel (i.e., multiple times) allows for amortizing the cost of the code dispatch. Moreover, in order to customize the dispatch mechanism, one can rely on the following interface.</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** If non-zero function pointer is returned, call (*function_ptr)(a, b, c). */</span>
libxsmm_smmfunction <span class="pl-en">libxsmm_smmdispatch</span>(<span class="pl-k">int</span> m, <span class="pl-k">int</span> n, <span class="pl-k">int</span> k,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* lda, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldb, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldc,
  <span class="pl-k">const</span> <span class="pl-k">float</span>* alpha, <span class="pl-k">const</span> <span class="pl-k">float</span>* beta,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* flags, <span class="pl-k">const</span> <span class="pl-k">int</span>* prefetch);
<span class="pl-c">/** If non-zero function pointer is returned, call (*function_ptr)(a, b, c). */</span>
libxsmm_dmmfunction <span class="pl-en">libxsmm_dmmdispatch</span>(<span class="pl-k">int</span> m, <span class="pl-k">int</span> n, <span class="pl-k">int</span> k,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* lda, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldb, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldc,
  <span class="pl-k">const</span> <span class="pl-k">double</span>* alpha, <span class="pl-k">const</span> <span class="pl-k">double</span>* beta,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* flags, <span class="pl-k">const</span> <span class="pl-k">int</span>* prefetch);</pre></div>

<p>A variety of overloaded function signatures is provided allowing to omit arguments not deviating from the configured defaults. In C++, a type <code>libxsmm_mmfunction&lt;type&gt;</code> can be used to instantiate a functor rather than making a distinction for the numeric type in <code>libxsmm_?mmdispatch</code>. Similarly in FORTRAN, when calling the generic interface (<code>libxsmm_mmdispatch</code>) the given <code>LIBXSMM_?MMFUNCTION</code> is dispatched such that <code>libxsmm_call</code> can be used to actually perform the function call using the PROCEDURE POINTER wrapped by <code>LIBXSMM_?MMFUNCTION</code>. Beside of dispatching code, one can also call a specific kernel (e.g., <code>libxsmm_dmm_4_4_4</code>) using the prototype functions included for statically generated kernels.</p>

<h2>
<a id="interface-for-convolutions" class="anchor" href="#interface-for-convolutions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interface for Convolutions</h2>

<p>In order to achieve best performance with small convolutions for CNN on SIMD architectures, a specific data layout has to be used. As this layout depends on several architectural parameters, the goal of LIBXSMM interface is to hide this complexity from the user by providing copy-in and copy-out routines. These happen on custom datatype which themselves are later bound to a convolution operation. The interface is available for C.</p>

<p>The main concept in LIBXSMM's frontend is that everything is circled around <code>libxsmm_dnn_conv_handle</code> which will define all properties of a layer operation. A handle can be created by describing the convolutional layer and calling a create function:</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** simplified LIBXSMM types which are needed to create a handle */</span>

<span class="pl-c">/** Structure which describes the input and output of data (DNN). */</span>
<span class="pl-k">typedef</span> <span class="pl-k">struct</span> LIBXSMM_RETARGETABLE libxsmm_dnn_conv_desc {
  <span class="pl-k">int</span> N;                                       <span class="pl-c">/* number of images in mini-batch */</span>
  <span class="pl-k">int</span> C;                                       <span class="pl-c">/* number of input feature maps */</span>
  <span class="pl-k">int</span> H;                                       <span class="pl-c">/* height of input image */</span>
  <span class="pl-k">int</span> W;                                       <span class="pl-c">/* width of input image */</span>
  <span class="pl-k">int</span> K;                                       <span class="pl-c">/* number of output feature maps */</span>
  <span class="pl-k">int</span> R;                                       <span class="pl-c">/* height of filter kernel */</span>
  <span class="pl-k">int</span> S;                                       <span class="pl-c">/* width of filter kernel */</span>
  <span class="pl-k">int</span> u;                                       <span class="pl-c">/* vertical stride */</span>
  <span class="pl-k">int</span> v;                                       <span class="pl-c">/* horizontal stride */</span>
  <span class="pl-k">int</span> pad_h_in;                                <span class="pl-c">/* height of zero-padding in input buffer, ignored */</span>
  <span class="pl-k">int</span> pad_w_in;                                <span class="pl-c">/* width of zero-padding in input buffer, ignored */</span>
  <span class="pl-k">int</span> pad_h_out;                               <span class="pl-c">/* height of zero-padding in output buffer */</span>
  <span class="pl-k">int</span> pad_w_out;                               <span class="pl-c">/* width of zero-padding in output buffer */</span>
  libxsmm_dnn_conv_algo algo;                  <span class="pl-c">/* convolution algorithm used */</span>
  libxsmm_dnn_conv_format buffer_format;       <span class="pl-c">/* format which is for buffer buffers */</span>
  libxsmm_dnn_conv_format filter_format;       <span class="pl-c">/* format which is for filter buffers */</span>
  libxsmm_dnn_conv_fuse_ops fuse_ops;          <span class="pl-c">/* used ops into convolutions */</span>
  libxsmm_dnn_datatype datatype;               <span class="pl-c">/* datatypes use for all buffers */</span>
} libxsmm_dnn_conv_desc;

<span class="pl-c">/** Type of algorithm used for convolutions. */</span>
<span class="pl-k">typedef</span> <span class="pl-k">enum</span> libxsmm_dnn_conv_algo {
  <span class="pl-c">/** direct convolution. */</span>
  LIBXSMM_DNN_CONV_ALGO_DIRECT
} libxsmm_dnn_conv_algo;

<span class="pl-c">/** Denotes the element/pixel type of an image/channel. */</span>
<span class="pl-k">typedef</span> <span class="pl-k">enum</span> libxsmm_dnn_conv_datatype {
  LIBXSMM_DNN_DATATYPE_F32
} libxsmm_dnn_datatype;

libxsmm_dnn_conv_handle* <span class="pl-en">libxsmm_dnn_create_conv_handle_check</span>(
  libxsmm_dnn_conv_desc   conv_desc,
  libxsmm_dnn_datatype    conv_datatype,
  libxsmm_dnn_conv_algo   conv_algo,
  <span class="pl-c1">libxsmm_dnn_err_t</span>*      status);</pre></div>

<p>Therefore, a sample call looks like:</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** Macro to check for an error. */</span>
#<span class="pl-k">define</span> <span class="pl-en">CHKERR_LIBXSMM_DNN</span>(<span class="pl-v">A</span>) <span class="pl-k">if</span> (A != LIBXSMM_DNN_SUCCESS) \
  <span class="pl-en">fprintf</span>(stderr, <span class="pl-s"><span class="pl-pds">"</span><span class="pl-c1">%s</span><span class="pl-cce">\n</span><span class="pl-pds">"</span></span>, libxsmm_dnn_get_error(A));
<span class="pl-c">/* declare LIBXSMM variables */</span>
libxsmm_dnn_conv_desc conv_desc;
<span class="pl-c1">libxsmm_dnn_err_t</span> status;
libxsmm_dnn_conv_handle* libxsmm_handle;
<span class="pl-c">/* setting conv_desc values.... */</span>
conv_desc.N = ...
<span class="pl-c">/* create handle */</span>
libxsmm_handle = libxsmm_dnn_create_conv_handle_check(conv_desc, &amp;status);
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(status);</pre></div>

<p>Next activation and filter buffers need to be created, initialized and bound to the handle. Afterwards the convolution could be executed by a threading environment of choice:</p>

<div class="highlight highlight-source-c"><pre>libxsmm_dnn_buffer* libxsmm_input;
libxsmm_dnn_buffer* libxsmm_output;
libxsmm_dnn_filter* libxsmm_filter;

<span class="pl-c">/* setup LIBXSMM layer information */</span>
libxsmm_input = libxsmm_dnn_create_input_buffer_check(libxsmm_handle, &amp;status);
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(status);
libxsmm_output = libxsmm_dnn_create_output_buffer_check(libxsmm_handle, &amp;status);
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(status);
libxsmm_filter = libxsmm_dnn_create_filter_check(libxsmm_handle, &amp;status);
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(status);

<span class="pl-c">/* copy in data to LIBXSMM format: naive format is: */</span>
<span class="pl-c">/* (mini-batch)(number-featuremaps)(featuremap-height)(featuremap-width) for layers, */</span>
<span class="pl-c">/* and the naive format for filters is: */</span>
<span class="pl-c">/* (number-output-featuremaps)(number-input-featuremaps)(kernel-height)(kernel-width) */</span>
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(libxsmm_dnn_copyin_buffer(libxsmm_input, (<span class="pl-k">void</span>*)naive_input));
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(libxsmm_dnn_zero_buffer(libxsmm_output));
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(libxsmm_dnn_copyin_filter(libxsmm_filter, (<span class="pl-k">void</span>*)naive_filter));

<span class="pl-c">/* bind layer to handle */</span>
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(libxsmm_dnn_bind_input_buffer(libxsmm_handle, libxsmm_input));
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(libxsmm_dnn_bind_output_buffer(libxsmm_handle, libxsmm_output));
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(libxsmm_dnn_bind_filter(libxsmm_handle, libxsmm_filter));

<span class="pl-c">/* run the convolution */</span>
#<span class="pl-k">pragma</span> omp parallel
{
  <span class="pl-c1">CHKERR_LIBXSMM_DNN</span>(<span class="pl-c1">libxsmm_dnn_convolve_st</span>(libxsmm_handle, LIBXSMM_DNN_CONV_KIND_FWD, <span class="pl-c1">0</span>,
    <span class="pl-c1">omp_get_thread_num</span>(), <span class="pl-c1">omp_get_num_threads</span>()));
}

<span class="pl-c">/* copy out data */</span>
<span class="pl-en">CHKERR_LIBXSMM_DNN</span>(libxsmm_dnn_copyout_buffer(libxsmm_output, (<span class="pl-k">void</span>*)naive_libxsmm_output));</pre></div>

<h2>
<a id="service-functions" class="anchor" href="#service-functions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Service Functions</h2>

<p>For convenient operation of the library and to ease integration, a number of service routines are available. They do not exactly belong to the core functionality of LIBXSMM (SMM or DNN domain), but users are encouraged to rely on these routines of the API. There are two categories: (1) routines which are available for C and Fortran, and (2) routines which are only available with the C interface.</p>

<h3>
<a id="getting-and-setting-the-target-architecture" class="anchor" href="#getting-and-setting-the-target-architecture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting and Setting the Target Architecture</h3>

<p>There are ID based and string based functions to query the code path (as determined by the CPUID), or to set the code path regardless of the presented CPUID features. The latter may degrade performance (if a lower set of instruction set extensions is requested), which can be still useful for studying the performance impact of different instruction set extensions. This functionality is available for the C and Fortran interface, and there is an environment variable which corresponds to <code>libxsmm_set_target_arch</code> (LIBXSMM_TARGET).<br>
<strong>NOTE</strong>: There is no additional check performed if an unsupported instruction set extension is requested, and incompatible JIT-generated code may be executed (unknown instruction signaled).</p>

<div class="highlight highlight-source-c"><pre><span class="pl-k">int</span> <span class="pl-en">libxsmm_get_target_archid</span>(<span class="pl-k">void</span>);
<span class="pl-k">void</span> <span class="pl-en">libxsmm_set_target_archid</span>(<span class="pl-k">int</span> id);

<span class="pl-k">const</span> <span class="pl-k">char</span>* <span class="pl-en">libxsmm_get_target_arch</span>(<span class="pl-k">void</span>);
<span class="pl-k">void</span> <span class="pl-en">libxsmm_set_target_arch</span>(<span class="pl-k">const</span> <span class="pl-k">char</span>* arch);</pre></div>

<h3>
<a id="getting-and-setting-the-verbosity" class="anchor" href="#getting-and-setting-the-verbosity" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Getting and Setting the Verbosity</h3>

<p>The <a href="#verbose-mode">Verbose Mode</a> (level of verbosity) can be controlled using the C or Fortran API, and there is an environment variable which corresponds to <code>libxsmm_set_verbosity</code> (LIBXSMM_VERBOSE).</p>

<div class="highlight highlight-source-c"><pre><span class="pl-k">int</span> <span class="pl-en">libxsmm_get_verbosity</span>(<span class="pl-k">void</span>);
<span class="pl-k">void</span> <span class="pl-en">libxsmm_set_verbosity</span>(<span class="pl-k">int</span> level);</pre></div>

<h3>
<a id="timer-facility" class="anchor" href="#timer-facility" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Timer Facility</h3>

<p>Due to the performance oriented nature of LIBXSMM, timer-related functionality is available for the C and Fortran interface. This is used for instance by the code samples, which measure the duration of executing various code regions. Both "tick" functions (<code>libxsmm_timer_[x]tick</code>) are based on monotonic timer sources, which use a platform-specific resolution. The xtick-counter attempts to directly rely on the time stamp counter instruction (RDTSC), but it is not necessarily counting real CPU cycles due to varying CPU clock speed (Turbo Boost), different clock domains (e.g., depending on the instructions executed), and other reasons (which are out of scope in this context).<br>
<strong>NOTE</strong>: <code>libxsmm_timer_xtick</code> is not directly suitable for <code>libxsmm_timer_duration</code> (seconds).</p>

<div class="highlight highlight-source-c"><pre><span class="pl-k">unsigned</span> <span class="pl-k">long</span> <span class="pl-k">long</span> <span class="pl-en">libxsmm_timer_tick</span>(<span class="pl-k">void</span>);
<span class="pl-k">unsigned</span> <span class="pl-k">long</span> <span class="pl-k">long</span> <span class="pl-en">libxsmm_timer_xtick</span>(<span class="pl-k">void</span>);
<span class="pl-k">double</span> <span class="pl-en">libxsmm_timer_duration</span>(<span class="pl-k">unsigned</span> <span class="pl-k">long</span> <span class="pl-k">long</span> tick0, <span class="pl-k">unsigned</span> <span class="pl-k">long</span> <span class="pl-k">long</span> tick1);</pre></div>

<h3>
<a id="memory-allocation" class="anchor" href="#memory-allocation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Memory Allocation</h3>

<p>Without further claims on the properties of the memory allocation (e.g., thread scalability), there are C functions that allocate aligned memory one of which allows to specify the alignment (or to specify an automatically chosen alignment). The automatic alignment is also exposed by a <code>malloc</code> compatible signature. The size of the automatic alignment depends on a heuristic, which uses the size of the requested buffer.<br>
<strong>NOTE</strong>: only <code>libxsmm_free</code> is supported in order to deallocate the memory.</p>

<div class="highlight highlight-source-c"><pre><span class="pl-k">void</span>* <span class="pl-en">libxsmm_aligned_malloc</span>(<span class="pl-c1">size_t</span> size, <span class="pl-k">int</span> alignment);
<span class="pl-k">void</span>* <span class="pl-en">libxsmm_malloc</span>(<span class="pl-c1">size_t</span> size);
<span class="pl-k">void</span> <span class="pl-en">libxsmm_free</span>(<span class="pl-k">const</span> <span class="pl-k">volatile</span> <span class="pl-k">void</span>* memory);</pre></div>

<h2>
<a id="build-instructions" class="anchor" href="#build-instructions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build Instructions</h2>

<p>The build system relies on GNU Make (typically associated with the <code>make</code> command, but e.g. FreeBSD is calling it <code>gmake</code>). The build can be customized by using key‑value pairs. Key‑value pairs can be supplied in two ways: (1) after the "make" command, or (2) prior to the "make" command (<code>env</code>) which is effectively the same as exporting the key‑value pair as an environment variable (<code>export</code>, or <code>setenv</code>). Of course both methods can be mixed, however the second method may require to supply the <code>-e</code> flag. Please note that the CXX, CC, and FC keys are handled such that they are taken into account in any case.</p>

<p>To generate the interface of the library inside of the 'include' directory and to build the static library (by default, STATIC=1 is activated), simply run the following command:</p>

<pre><code>make
</code></pre>

<p>If the build process is not successful, it may help to avoid more advanced GCC flags. This is useful with a tool chain, which pretends to be GCC-compatible (or is treated as such) but actually fails to consume the aforementioned flags. In such a case (CCE, etc.) one may raise the compatibility:</p>

<pre><code>make COMPATIBLE=1
</code></pre>

<p>By default, only the non-coprocessor targets are built (OFFLOAD=0 and KNC=0). In general, the subfolders of the 'lib' directory are separating the build targets where the 'mic' folder is containing the native library (KNC=1) targeting the Intel Xeon Phi coprocessor ("KNC"), and the 'intel64' folder is storing either the hybrid archive made of CPU and coprocessor code (OFFLOAD=1), or an archive which is only containing the CPU code. By default, an OFFLOAD=1 implies KNC=1.</p>

<p>To remove intermediate files, or to remove all generated files and folders (including the interface and the library archives), run one of the following commands:</p>

<pre><code>make clean
make realclean
</code></pre>

<p>By default, LIBXSMM uses the <a href="#jit-backend">JIT backend</a> which is automatically building optimized code. However, one can also statically specialize for particular matrix sizes (M, N, and K values), for convolutions the options below can be ignored:</p>

<pre><code>make M="2 4" N="1" K="$(echo $(seq 2 5))"
</code></pre>

<p>The above example is generating the following set of (M,N,K) triplets:</p>

<pre><code>(2,1,2), (2,1,3), (2,1,4), (2,1,5),
(4,1,2), (4,1,3), (4,1,4), (4,1,5)
</code></pre>

<p>The index sets are in a loop-nest relationship (M(N(K))) when generating the indices. Moreover, an empty index set resolves to the next non-empty outer index set of the loop nest (including to wrap around from the M to K set). An empty index set is not participating anymore in the loop-nest relationship. Here is an example of generating multiplication routines which are "squares" with respect to M and N (N inherits the current value of the "M loop"):</p>

<pre><code>make M="$(echo $(seq 2 5))" K="$(echo $(seq 2 5))"
</code></pre>

<p>An even more flexible specialization is possible by using the MNK variable when building the library. It takes a list of indexes which are eventually grouped (using commas):</p>

<pre><code>make MNK="2 3, 23"
</code></pre>

<p>Each group of the above indexes is combined into all possible triplets generating the following set of (M,N,K) values:</p>

<pre><code>(2,2,2), (2,2,3), (2,3,2), (2,3,3),
(3,2,2), (3,2,3), (3,3,2), (3,3,3), (23,23,23)
</code></pre>

<p>Of course, both mechanisms (M/N/K and MNK based) can be combined using the same command line (make). Static optimization and JIT can also be combined (no need to turn off the JIT backend). Testing the library is supported by a variety of targets with "test" and "test-all" being the most prominent for this matter.</p>

<p>Functionality of LIBXSMM, which is unrelated to GEMM can be used without introducing a dependency to BLAS. This can be achieved in two ways: (1) building a special library with <code>make BLAS=0</code>, or (2) linking the application against the 'libxsmmnoblas' library. Some care must be taken with any matrix multiplication which does not appear to require BLAS for the given test arguments. However, it may fall back to BLAS (at runtime of the application), if an unforeseen input is given (problem size, or unsupported GEMM arguments).</p>

<p><strong>NOTE</strong>: by default, a C/C++ and a FORTRAN compiler is needed (some sample code is written in C++). Beside of specifying the compilers (<code>make CXX=g++ CC=gcc FC=gfortran</code> and maybe <code>AR=ar</code>), the need for a FORTRAN compiler can be relaxed (<code>make FC=</code> or <code>make FORTRAN=0</code>). The latter affects the availability of the MODule file and the corresponding 'libxsmmf' library (the interface 'libxsmm.f' is still generated). FORTRAN code can make use of LIBXSMM in three different ways:</p>

<ul>
<li>By relying on the module file, and by linking against 'libxsmmf', 'libxsmm', and (optionally) 'libxsmmext',</li>
<li>By including the interface 'libxsmm.f' and linking against 'libxsmm', and (optionally) 'libxsmmext', or</li>
<li>By declaring e.g., <code>libxsmm_?gemm</code> (BLAS signature) and linking 'libxsmm' (and 'libxsmmext' if needed).</li>
</ul>

<p>At the expense of a limited set of functionality (<code>libxsmm_?gemm[_omp]</code>, <code>libxsmm_blas_?gemm</code>, and <code>libxsmm_[s|d]otrans[_omp]</code>), the latter method also works with FORTRAN 77 (otherwise the FORTRAN 2003 standard is necessary). For the "omp" functionality, the 'libxsmmext' library needs to be present at the link line. For no code change at all, the <a href="#call-wrapper">Call Wrapper</a> might be of interest.</p>

<h2>
<a id="link-instructions" class="anchor" href="#link-instructions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Link Instructions</h2>

<p>The library is agnostic with respect to the threading-runtime, and therefore an application is free to use any threading runtime (e.g., OpenMP). The library is also thread-safe, and multiple application threads can call LIBXSMM's routines concurrently. Forcing OpenMP (OMP=1) for the entire build of LIBXSMM is not supported and untested ('libxsmmext' is automatically built with OpenMP enabled).</p>

<p>Similarly, an application is free to choose any BLAS or LAPACK library (if the link model available on the OS supports this), and therefore linking GEMM routines when linking LIBXSMM itself (by supplying BLAS=1|2) may prevent a user from making this decision at the time of linking the actual application.</p>

<p><strong>NOTE</strong>: LIBXSMM does not support to dynamically link against 'libxsmm' or 'libxsmmext' ("so"), when BLAS is linked statically ("a").</p>

<h2>
<a id="installation" class="anchor" href="#installation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

<p>Installing LIBXSMM makes possibly the most sense when combining the <a href="#jit-backend">JIT backend</a> (enabled by default) with a collection of statically generated SSE kernels (by specifying M, N, K, or MNK). If the JIT backend is not disabled, statically generated kernels are only registered for dispatch if the CPUID flags at runtime are not supporting a more specific instruction set extension (code path). Since the JIT backend does not support or generate SSE code by itself, the library is compiled by selecting SSE code generation if not specified otherwise (AVX=1|2|3, or with SSE=0 falling back to an "arch-native" approach). Limiting the static code path to SSE3 (SSE4.2 under OS X) allows to practically target any deployed system, however using SSE=0 and AVX=0 together is falling back to generic code, and any static kernels are not specialized using the assembly code generator.</p>

<p>There are two main mechanisms to install LIBXSMM (both mechanisms can be combined): (1) building the library in an out‑of‑tree fashion, and (2) installing into a certain location. Building in an out‑of‑tree fashion looks like:</p>

<pre><code>cd libxsmm-install
make -f /path/to/libxsmm/Makefile
</code></pre>

<p>For example, installing into a specific location (incl. a selection of statically generated Intel SSE kernels) looks like:</p>

<pre><code>make MNK="1 2 3 4 5" PREFIX=/path/to/libxsmm-install install
</code></pre>

<p>Performing <code>make install-minimal</code> omits the documentation (default: 'PREFIX/share/libxsmm'). Moreover, PINCDIR, POUTDIR, PBINDIR, and PDOCDIR allow to customize the locations underneath of the PREFIX location. To build a general package for an unpredictable audience (Linux distribution, or similar), it is advised to not over-specify or customize the build step i.e., JIT, SSE, AVX, OMP, BLAS, etc. should not be used. The following is building and installing a complete set of libraries where the generated interface matches both the static and the shared libraries:</p>

<pre><code>make PREFIX=/path/to/libxsmm-install STATIC=0 install
make PREFIX=/path/to/libxsmm-install install
</code></pre>

<h2>
<a id="running" class="anchor" href="#running" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running</h2>

<h3>
<a id="call-wrapper" class="anchor" href="#call-wrapper" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Call Wrapper</h3>

<p>Since the library is binary compatible with existing GEMM calls (BLAS), these calls can be replaced at link-time or intercepted at runtime of an application such that LIBXSMM is used instead of the original BLAS library. There are two cases to consider:</p>

<ul>
<li>An application which is linked statically against BLAS requires to wrap the 'sgemm_' and the 'dgemm_' symbol (an alternative is to wrap only 'dgemm_'), and a special build of the libxsmm(ext) library is required (<code>make WRAP=1</code> to to wrap SGEMM and DGEMM, or <code>make WRAP=2</code> to wrap only DGEMM):<br>
<code>gcc [...] -Wl,--wrap=sgemm_,--wrap=dgemm_ /path/to/libxsmmext.a /path/to/libxsmm.a /path/to/your_regular_blas.a</code><br>
Relinking the application as shown above can often be accomplished by copying, pasting, modifying the linker command, and then re-invoking the modified link step. This linker command may appear as console output of the application's "make" command (or a similar build system).<br>
The static link-time wrapper technique may only work with a GCC tool chain (GNU Binutils: <code>ld</code>, or <code>ld</code> via compiler-driver), and it has been tested with GNU GCC, Intel Compiler, and Clang. However, this does not work under Microsoft Windows (even when using the GNU tool chain), and it may not work under OS X (Compiler 6.1 or earlier, later versions have not been tested).</li>
<li>An application which is dynamically linked against BLAS allows for intercepting the GEMM calls at startup time (runtime) of the unmodified executable by using the LD_PRELOAD mechanism. The shared library of LIBXSMM (<code>make STATIC=0</code>) allows to intercept the GEMM calls of the application:<br>
<code>LD_PRELOAD=/path/to/libxsmmext.so ./myapplication</code>
</li>
</ul>

<p>The behavior of the intercepted GEMM routines (statically wrapped or via LD_PRELOAD) can be controlled with the environment variable LIBXSMM_MT i.e., 0: calling sequential below-threshold routines without OpenMP (default when only linking 'libxsmm'), 1: OpenMP-parallelized behavior but without an internal parallel region, and 2: OpenMP-parallelized routines with internal parallel region (default when linking 'libxsmmext'). In any case, the wrapper mechanism also supports to fall back to BLAS.</p>

<pre><code>LIBXSMM_MT=0 ./myapplication
</code></pre>

<p><strong>NOTE</strong>: Using the same multiplication kernel in a consecutive fashion (batch-processing) allows to extract higher performance, when using LIBXSMM's native programming interface.</p>

<h3>
<a id="verbose-mode" class="anchor" href="#verbose-mode" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Verbose Mode</h3>

<p>The verbose mode allows for an insight into the code dispatch mechanism by receiving a small tabulated statistic as soon as the library terminates. The design point for this functionality is to not impact the performance of any critical code path i.e., verbose mode is always enabled and does not require symbols (SYM=1) or debug code (DBG=1). The statistics appears (<code>stderr</code>) when the environment variable LIBXSMM_VERBOSE is set to a non-zero value. For example:</p>

<pre><code>LIBXSMM_VERBOSE=1 ./myapplication
[... application output]

HSW/SP        TRY    JIT    STA    COL
     0..13      7      7      0      0
    14..23      0      0      0      0
    24..80      3      3      0      0
</code></pre>

<p>The tables are distinct between single-precision and double-precision, but either table is pruned if all counters are zero. If both tables are pruned, the library shows the code path which would have been used for JIT'ting the code: <code>LIBXSMM_TARGET=hsw</code> (otherwise the code path is shown in the table's header). The actual counters are collected for three buckets: small kernels (MNK<sup>1/3</sup> &lt;= 13), medium-sized kernels (13 &lt; MNK<sup>1/3</sup> &lt;= 23), and larger kernels (23 &lt; MNK<sup>1/3</sup> &lt;= 80; the actual upper bound depends on LIBXSMM_MAX_MNK as selected at compile-time). Keep in mind, that "larger" is supposedly still fairly small in terms of arithmetic intensity (which grows linearly with the kernel size). Unfortunately, the arithmetic intensity depends on the way a kernel is used (which operands are loaded/stored into main memory) and it is not performance-neutral to collect this information.</p>

<p>The TRY counter represents all attempts to register statically generated kernels, and all attempts to dynamically generate and register kernels. The TRY counter includes rejected JIT requests due to unsupported GEMM arguments. The JIT and STA counters distinct the successful cases of the aforementioned event (TRY) into dynamically (JIT) and statically (STA) generated code. In case the capacity (O(<em>n</em>) = 10<sup>5</sup>) of the code registry is exhausted, no more kernels can be registered although further attempts are not prevented. Registering many kernels (O(<em>n</em>) = 10<sup>3</sup>) may ramp the number of hash key collisions (COL), which can degrade performance. The latter is prevented if the small thread-local cache is utilized effectively.</p>

<p>Since explicitly JIT-generated code (<code>libxsmm_?mmdispatch</code>) does not fall under the THRESHOLD criterion, the above table is extended by one line if large kernels have been requested. This indicates a missing threshold-criterion (customized dispatch), or asks for cache-blocking the matrix multiplication. The latter is already implemented by LIBXSMM's "medium-sized" GEMM routines (<code>libxsmm_?gemm_omp</code>), which perform a tiled multiplication.</p>

<p><strong>NOTE</strong>: setting LIBXSMM_VERBOSE to a negative value will dump each generated JIT kernel to a file with each file being named similar to the function name shown in <a href="#profiling">Intel VTune</a>.</p>

<h3>
<a id="call-trace" class="anchor" href="#call-trace" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Call Trace</h3>

<p>During the initial steps of employing the LIBXSMM API, one may rely on a debug version of the library (<code>make DBG=1</code>). The latter also implies console output (<code>stderr</code>) in case of an error/warning condition inside of the library. It is also possible to print the execution flow (call trace) inside of LIBXSMM (can be combined with DBG=1 or OPT=0):</p>

<pre><code>make TRACE=1
</code></pre>

<p>Building an application which is able to trace calls (inside of the library) requires the shared library of LIBXSMM, alternatively the application is required to link the static library of LIBXSMM in a dynamic fashion (GNU tool chain: <code>-rdynamic</code>). Actually tracing calls (without debugger) can be the accomplished by an environment variable called LIBXSMM_TRACE.</p>

<pre><code>LIBXSMM_TRACE=1 ./myapplication
</code></pre>

<p>Syntactically up to three arguments separated by commas (which allows to omit arguments) are taken (<em>tid</em>,<em>i</em>,<em>n</em>): <em>tid</em> signifies the ID of the thread to be traced with 1...NTHREADS being valid and where LIBXSMM_TRACE=1 is filtering for the "main thread" (in fact the first thread running into the trace facility); grabbing all threads (no filter) can be achieved by supplying a negative id (which is also the default when omitted). The second argument is pruning higher levels of the call-tree with <em>i=1</em> being the default (level zero is the highest at the same level as the main function). The last argument is taking the number of inclusive call levels with <em>n=-1</em> being the default (signifying no filter).</p>

<p>Although the <code>ltrace</code> (Linux utility) provides similar insight, the trace facility might be useful due to the aforementioned filtering expressions. Please note that the trace facility is severely impacting the performance (even with LIBXSMM_TRACE=0), and this is not just because of console output but rather since inlining (internal) functions might be prevented along with additional call overhead on each function entry and exit. Therefore, debug symbols can be also enabled separately (<code>make SYM=1</code>; implied by TRACE=1 or DBG=1) which might be useful when profiling an application. No facility of the library (other than DBG or TRACE/LIBXSMM_TRACE) is performing visible (console) or other non-private I/O (files).</p>

<h2>
<a id="performance" class="anchor" href="#performance" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performance</h2>

<h3>
<a id="profiling" class="anchor" href="#profiling" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Profiling</h3>

<h4>
<a id="intelvtuneamplifier" class="anchor" href="#intelvtuneamplifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Intel VTune Amplifier</h4>

<p>To analyze which kind of kernels have been called, and from where these kernels have been invoked (call stack), the library allows profiling its JIT code using Intel VTune Amplifier. To enable this support, VTune's root directory needs to be set at build-time of the library. Enabling symbols (SYM=1 or DBG=1) incorporates VTune's JIT Profiling API:</p>

<pre><code>source /path/to/vtune_amplifier/amplxe-vars.sh
make SYM=1
</code></pre>

<p>Above, the root directory is automatically determined from the environment (VTUNE_AMPLIFIER_*_DIR). This variable is present after source'ing the Intel VTune environment, but it can be manually provided as well (<code>make VTUNEROOT=/path/to/vtune_amplifier</code>). Symbols are actually not required to display kernel names for the dynamically generated code, however enabling symbols makes the analysis much more useful for the rest of the (static) code, and hence it has been made a prerequisite. For example, when "call stacks" are collected it is possible to find out where the JIT code has been invoked by the application:</p>

<pre><code>amplxe-cl -r result-directory -data-limit 0 -collect advanced-hotspots \
          -knob collection-detail=stack-sampling -- ./myapplication
</code></pre>

<p>In case of an MPI-parallelized application, it might be useful to only collect results from a "representative" rank, and to also avoid running the event collector in every rank of the application. With Intel MPI both of the latter can be achieved by adding</p>

<pre><code>-gtool 'amplxe-cl -r result-directory -data-limit 0 -collect advanced-hotspots \
                  -knob collection-detail=stack-sampling:4=exclusive'
</code></pre>

<p>to the <code>mpirun</code> command line. Please notice the <code>:4=exclusive</code> (unrelated to VTune's command line syntax), which is related to mpirun's gtool arguments; these arguments need to appear at the end of the gtool-string. For instance, the shown command line selects the 4th rank (otherwise all ranks are sampled) along with "exclusive" usage of the performance monitoring unit (PMU) such that only one event-collector runs for all ranks.</p>

<p>Intel VTune Amplifier presents invoked JIT code like functions, which belong to a module named "libxsmm.jit". The function name as well as the module name are supplied by LIBXSMM using the aforementioned JIT Profiling API. For instance "libxsmm_hsw_dnn_23x23x23_23_23_23_a1_b1_p0::smxm" encodes an Intel AVX2 ("hsw") double-precision kernel ("d") for small dense matrix multiplications ("smxm") which is multiplying matrices without transposing them ("nn"). The rest of the name encodes M=N=K=LDA=LDB=LDC=23, Alpha=Beta=1.0 (all similar to GEMM), and no prefetch strategy ("p0").</p>

<h4>
<a id="linux-perf" class="anchor" href="#linux-perf" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Linux perf</h4>

<p>With LIBXSMM, there is both basic (<code>perf map</code>) and extended support (<code>jitdump</code>) when profiling an application. To enable perf support at runtime, the environment LIBXSMM_VERBOSE needs to be set to a negative value.</p>

<ul>
<li>The basic support can be enabled at compile-time with PERF=1 (implies SYM=1) using <code>make PERF=1</code>. At runtime of the application, a map-file ('jit-<em>pid</em>.map') is generated ('/tmp' directory). This file is automatically read by Linxu perf, and enriches the information about unknown code such as JIT'ted kernels.</li>
<li>The support for "jitdump" can be enabled by supplying JITDUMP=1 (implies PERF=1) or PERF=2 (implies JITDUMP=1) when making the library: <code>make JITDUMP=1</code> or <code>make PERF=2</code>. At runtime of the application, a dump-file ('jit-<em>pid</em>.dump') is generated (in perf's debug directory, usually <code>$HOME/.debug/jit/</code>) which includes information about JIT'ted kernels (such as addresses, symbol names, code size, and the code itself). The dump file can be injected into 'perf.data' (using <code>perf inject -j</code>), and it enables an annotated view of the assembly in perf's report (requires a reasonably recent version of perf).</li>
</ul>

<h3>
<a id="tuning" class="anchor" href="#tuning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tuning</h3>

<p>Specifying a particular code path is not really necessary if the JIT backend is not disabled. However, disabling JIT compilation, statically generating a collection of kernels, and targeting a specific instruction set extension for the entire library looks like:</p>

<pre><code>make JIT=0 AVX=3 MNK="1 2 3 4 5"
</code></pre>

<p>The above example builds a library which cannot be deployed to anything else but the Intel Knights Landing processor family ("KNL") or future Intel Xeon processors supporting foundational Intel AVX‑512 instructions (AVX‑512F). The latter might be even more adjusted by supplying MIC=1 (along with AVX=3), however this does not matter since critical code is in inline assembly (and not affected). Similarly, SSE=0 (or JIT=0 without SSE or AVX build flag) employs an "arch-native" approach whereas AVX=1, AVX=2 (with FMA), and AVX=3 are specifically selecting the kind of Intel AVX code. Moreover, controlling the target flags manually or adjusting the code optimizations is also possible. The following example is GCC-specific and corresponds to OPT=3, AVX=3, and MIC=1:</p>

<pre><code>make OPT=3 TARGET="-mavx512f -mavx512cd -mavx512er -mavx512pf"
</code></pre>

<p>An extended interface can be generated which allows to perform software prefetches. Prefetching data might be helpful when processing batches of matrix multiplications where the next operands are farther away or otherwise unpredictable in their memory location. The prefetch strategy can be specified similar as shown in the section <a href="#generator-driver">Generator Driver</a> i.e., by either using the number of the shown enumeration, or by exactly using the name of the prefetch strategy. The only exception is PREFETCH=1 which is automatically selecting a strategy according to an internal table (navigated by CPUID flags). The following example is requesting the "AL2jpst" strategy:</p>

<pre><code>make PREFETCH=8
</code></pre>

<p>The prefetch interface is extending the signature of all kernels by three arguments (pa, pb, and pc). These additional arguments are specifying the locations of the operands of the next multiplication (the next a, b, and c matrices). Providing unnecessary arguments in case of the three-argument kernels is not big a problem (beside of some additional call-overhead), however running a kernel which is picking up more than three arguments and actually picking up garbage data is disabling the hardware prefetcher (due to software prefetches) followed by a misleading prefetch location plus an eventual page fault due to an out-of-bounds (garbage-)location.</p>

<p>Further, the generated configuration (<a href="https://github.com/hfp/libxsmm/blob/master/src/template/libxsmm_config.h">template</a>) of the library encodes the parameters for which the library was built for (static information). This helps optimizing client code related to the library's functionality. For example, the LIBXSMM_MAX_* and LIBXSMM_AVG_* information can be used with the LIBXSMM_PRAGMA_LOOP_COUNT macro in order to hint loop trip counts when handling matrices related to the problem domain of LIBXSMM.</p>

<h3>
<a id="auto-dispatch" class="anchor" href="#auto-dispatch" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Auto-dispatch</h3>

<p>The function <code>libxsmm_?mmdispatch</code> helps amortizing the cost of the dispatch when multiple calls with the same M, N, and K are needed. The automatic code dispatch is orchestrating two levels:</p>

<ol>
<li>Specialized routine (implemented in assembly code),</li>
<li>BLAS library call (fallback).</li>
</ol>

<p>Both levels are accessible directly (see <a href="#interface">Interface</a> section) allowing to customize the code dispatch. The fallback level may be supplied by the Intel Math Kernel Library (Intel MKL) 11.2 DIRECT CALL feature.</p>

<p>Further, a preprocessor symbol denotes the largest problem size (<em>M</em> x <em>N</em> x <em>K</em>) that belongs to the first level, and therefore determines if a matrix multiplication falls back to BLAS. The problem size threshold can be configured by using for example:</p>

<pre><code>make THRESHOLD=$((60 * 60 * 60))
</code></pre>

<p>The maximum of the given threshold and the largest requested specialization refines the value of the threshold. Please note that explicitly JIT'ting and executing a kernel is possible and independent of the threshold. If a problem size is below the threshold, dispatching the code requires to figure out whether a specialized routine exists or not.</p>

<p>In order to minimize the probability of key collisions (code cache), the preferred precision of the statically generated code can be selected:</p>

<pre><code>make PRECISION=2
</code></pre>

<p>The default preference is to generate and register both single and double-precision code, and therefore no space in the dispatch table is saved (PRECISION=0). Specifying PRECISION=1|2 is only generating and registering either single-precision or double-precision code.</p>

<p>The automatic dispatch is highly convenient because existing GEMM calls can serve specialized kernels (even in a binary compatible fashion), however there is (and always will be) an overhead associated with looking up the code-registry and checking whether the code determined by the GEMM call is already JIT'ted or not. This lookup has been optimized using various techniques such as using specialized CPU instructions calculating a CRC32 checksum, avoiding costly synchronization (needed for thread-safety) until it is ultimately known that the requested kernel is not yet JIT'ted, and also a small thread-local cache of recently dispatched kernels. The latter of which can be adjusted in size (only power-of-two sizes) but also disabled:</p>

<pre><code>make CACHE=0
</code></pre>

<p>Please note that measuring the relative cost of automatically dispatching a requested kernel depends on the kernel size (obviously smaller matrices are multiplied faster on an absolute basis), however smaller matrix multiplications are bottlenecked by memory bandwidth rather than arithmetic intensity. The latter implies the highest relative overhead when (artificially) benchmarking the very same multiplication out of the CPU-cache.</p>

<h3>
<a id="jit-backend" class="anchor" href="#jit-backend" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>JIT Backend</h3>

<p>There might be situations in which it is up-front not clear which problem sizes will be needed when running an application. In order to leverage LIBXSMM's high-performance kernels, the library implements a JIT (Just-In-Time) code generation backend which generates the requested kernels on the fly (in-memory). This is accomplished by emitting the corresponding byte-code directly into an executable buffer. The actual JIT code is generated according to the CPUID flags, and therefore does not rely on the code path selected when building the library. In the current implementation, some limitations apply to the JIT backend specifically:</p>

<ol>
<li>In order to stay agnostic to any threading model used, Pthread mutexes are guarding the updates of the JIT'ted code cache (link line with <code>-lpthread</code> is required); building with OMP=1 employs an OpenMP critical section as an alternative locking mechanism.</li>
<li>There is no support for the Intel SSE (Intel Xeon 5500/5600 series) and IMCI (Intel Xeon Phi coprocessor code-named Knights Corner) instruction set extensions. However, statically generated SSE-kernels can be leveraged without disabling support for JIT'ting AVX kernels.</li>
<li>There is no support for the Windows calling convention (only kernels with PREFETCH=0 signature).</li>
</ol>

<p>The JIT backend can also be disabled at build time (<code>make JIT=0</code>) as well as at runtime (<code>LIBXSMM_TARGET=0</code>, or anything prior to Intel AVX). The latter is an environment variable which allows to set a code path independent of the CPUID (LIBXSMM_TARGET=0|1|sse|snb|hsw|knl|skx). Please note that LIBXSMM_TARGET cannot enable the JIT backend if it was disabled at build time (JIT=0).</p>

<p>One can use the aforementioned THRESHOLD parameter to control the matrix sizes for which the JIT compilation will be automatically performed. However, explicitly requested kernels (by calling <code>libxsmm_?mmdispatch</code>) are not subject to a problem size threshold. In any case, JIT code generation can be used for accompanying statically generated code.</p>

<p>Note: Modern Linux kernels are supporting transparent huge pages (THP). LIBXSMM is sanitizing this feature when setting the permissions for pages holding the executable code. However, we measured up to 30% slowdown when running JIT'ted code in cases where THP decided to deliver a huge page. For systems with Linux kernel 2.6.38 (or later) THP will be automatically disabled for the <code>mmap</code>'ed regions (using <code>madvise</code>).</p>

<h3>
<a id="generator-driver" class="anchor" href="#generator-driver" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Generator Driver</h3>

<p>In rare situations it might be useful to directly incorporate generated C code (with inline assembly regions). This is accomplished by invoking a driver program (with certain command line arguments). The driver program is built as part of LIBXSMM's build process (when requesting static code generation), but also available via a separate build target:</p>

<pre><code>make generator
bin/libxsmm_gemm_generator
</code></pre>

<p>The code generator driver program accepts the following arguments:</p>

<ol>
<li>dense/dense_asm/sparse (dense creates C code, dense_asm creates ASM)</li>
<li>Filename of a file to append to</li>
<li>Routine name to be created</li>
<li>M parameter</li>
<li>N parameter</li>
<li>K parameter</li>
<li>LDA (0 when 1. is "sparse" indicates A is sparse)</li>
<li>LDB (0 when 1. is "sparse" indicates B is sparse)</li>
<li>LDC parameter</li>
<li>alpha (1)</li>
<li>beta (0 or 1)</li>
<li>Alignment override for A (1 auto, 0 no alignment)</li>
<li>Alignment override for C (1 auto, 0 no alignment)</li>
<li>Architecture (noarch, wsm, snb, hsw, knc, knl, skx)</li>
<li>Prefetch strategy, see below enumeration (dense/dense_asm only)</li>
<li>single precision (SP), or double precision (DP)</li>
<li>CSC file (just required when 1. is "sparse"). Matrix market format.</li>
</ol>

<p>The prefetch strategy can be:</p>

<ol>
<li>"nopf": no prefetching at all, just 3 inputs (A, B, C)</li>
<li>"pfsigonly": just prefetching signature, 6 inputs (A, B, C, A', B', C')</li>
<li>"BL2viaC": uses accesses to C to prefetch B'</li>
<li>"curAL2": prefetches current A ahead in the kernel</li>
<li>"curAL2_BL2viaC": combines curAL2 and BL2viaC</li>
<li>"AL2": uses accesses to A to prefetch A'</li>
<li>"AL2_BL2viaC": combines AL2 and BL2viaC</li>
<li>"AL2jpst": aggressive A' prefetch of first rows without any structure</li>
<li>"AL2jpst_BL2viaC": combines AL2jpst and BL2viaC</li>
<li>"AL2_BL2viaC_CL2": combines AL2 and BL2viaC</li>
</ol>

<p>Here are some examples of invoking the driver program:</p>

<pre><code>bin/libxsmm_gemm_generator dense foo.c foo 16 16 16 32 32 32 1 1 1 1 hsw nopf DP
bin/libxsmm_gemm_generator dense_asm foo.c foo 16 16 16 32 32 32 1 1 1 1 knl AL2_BL2viaC DP
bin/libxsmm_gemm_generator sparse foo.c foo 16 16 16 32 0 32 1 1 1 1 hsw nopf DP bar.csc
</code></pre>

<p>Please note, there are additional examples given in samples/generator and samples/seissol.</p>

<h3>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h3>

<p>The LIBXSMM repository provides an orphaned branch "results" which is collecting collateral material such as measured performance results along with explanatory figures. The results can be found at <a href="https://github.com/hfp/libxsmm/tree/results#libxsmm-results">https://github.com/hfp/libxsmm/tree/results#libxsmm-results</a>.</p>

<p>Please note that comparing performance results depends on whether or not streaming the operands of the matrix multiplication. For example, running a matrix multiplication code many time with all operands covered by the L1 cache may have an emphasis towards an implementation which actually performs worse for the real workload (if this real workload needs to stream some or all operands from the main memory).</p>

<h2>
<a id="contributions" class="anchor" href="#contributions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributions</h2>

<p>Contributions are very welcome! Please visit <a href="https://github.com/hfp/libxsmm/wiki/Contribute">https://github.com/hfp/libxsmm/wiki/Contribute</a>.</p>

<h2>
<a id="applications" class="anchor" href="#applications" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Applications</h2>

<p><strong>[1] <a href="https://cp2k.org/">https://cp2k.org/</a></strong>: Open Source Molecular Dynamics with its DBCSR component processing batches of small matrix multiplications ("matrix stacks") out of a problem-specific distributed block-sparse matrix. Starting with <a href="https://www.cp2k.org/version_history">CP2K 3.0</a>, LIBXSMM can be used to substitute CP2K's 'libsmm' library. Prior to CP2K 3.0, only the <a href="https://github.com/cp2k/cp2k/tree/intel">Intel-branch of CP2K</a> was integrating LIBXSMM (see <a href="https://github.com/hfp/libxsmm/raw/master/documentation/cp2k.pdf">https://github.com/hfp/libxsmm/raw/master/documentation/cp2k.pdf</a>).</p>

<p><strong>[2] <a href="https://github.com/SeisSol/SeisSol/">https://github.com/SeisSol/SeisSol/</a></strong>: SeisSol is one of the leading codes for earthquake scenarios, in particular for simulating dynamic rupture processes. LIBXSMM provides highly optimized assembly kernels which form the computational back-bone of SeisSol (see <a href="https://github.com/TUM-I5/seissol_kernels/">https://github.com/TUM-I5/seissol_kernels/</a>).</p>

<p><strong>[3] <a href="https://github.com/Nek5000/NekBox">https://github.com/Nek5000/NekBox</a></strong>: NekBox is a version of the highly scalable and portable spectral element <a href="https://nek5000.mcs.anl.gov/">Nek5000</a> code which is specialized for box geometries, and intended for prototyping new methods as well as leveraging FORTRAN beyond the FORTRAN 77 standard. LIBXSMM provides optimized kernels aiming to conveniently substitute the <a href="https://github.com/Nek5000/NekBox/blob/box/mxm_std.F90">MXM_STD</a> code.</p>

<p><strong>[4] <a href="https://github.com/Nek5000/Nek5000">https://github.com/Nek5000/Nek5000</a></strong>: Nek5000 is the open-source, highly-scalable, always-portable spectral element code from <a href="https://nek5000.mcs.anl.gov/">https://nek5000.mcs.anl.gov/</a>. The development branch of the Nek5000 code now <a href="https://github.com/Nek5000/Nek5000/blob/develop/core/mxm_wrapper.f">incorporates</a> LIBXSMM.</p>

<p><strong>[5] <a href="https://software.intel.com/en-us/articles/intel-xeon-phi-delivers-competitive-performance-for-deep-learning-and-getting-better-fast">https://software.intel.com/en-us/articles/intel-xeon-phi-delivers-competitive-performance-for-deep-learning-and-getting-better-fast</a></strong>: Intel Xeon Phi Delivers Competitive Performance For Deep Learning - And Getting Better Fast. Article mentioning LIBXSMM's performance of convolution kernels with DeepBench. Intel Corporation, 2016.</p>

<h2>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

<p><strong>[1] <a href="http://sc16.supercomputing.org/presentation/?id=pap364&amp;sess=sess153">http://sc16.supercomputing.org/presentation/?id=pap364&amp;sess=sess153</a></strong>: LIBXSMM: Accelerating Small Matrix Multiplications by Runtime Code Generation (<a href="http://www.computer.org/csdl/proceedings/sc/2016/8815/00/8815a981.pdf">paper</a>). SC'16: The International Conference for High Performance Computing, Networking, Storage and Analysis, Salt Lake City (Utah).</p>

<p><strong>[2] <a href="http://sc15.supercomputing.org/sites/all/themes/SC15images/tech_poster/tech_poster_pages/post137.html">http://sc15.supercomputing.org/sites/all/themes/SC15images/tech_poster/tech_poster_pages/post137.html</a></strong>: LIBXSMM: A High Performance Library for Small Matrix Multiplications (<a href="http://sc15.supercomputing.org/sites/all/themes/SC15images/tech_poster/poster_files/post137s2-file2.pdf">poster</a> and <a href="http://sc15.supercomputing.org/sites/all/themes/SC15images/tech_poster/poster_files/post137s2-file3.pdf">abstract</a>). SC'15: The International Conference for High Performance Computing, Networking, Storage and Analysis, Austin (Texas).</p>
      </section>
      <footer>
        <p><small>Hosted on <a href="https://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-65670560-2");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
