{"name":"LIBXSMM","tagline":"Library for small matrix-matrix multiplications targeting Intel Architecture (x86).","body":"# LIBXSMM\r\nLIBXSMM is a library for small matrix-matrix multiplications targeting Intel Architecture (x86). The library generates code for the following instruction set extensions: Intel SSE3, Intel AVX, Intel AVX2, IMCI (KNCni) for Intel Xeon Phi coprocessors (\"KNC\"), and Intel AVX-512 as found in the Intel Xeon Phi processor family (\"KNL\") and future Intel Xeon processors. Historically the library was solely targeting the Intel Many Integrated Core Architecture \"MIC\") using intrinsic functions, meanwhile optimized assembly code is targeting all fore mentioned instruction set extensions. [[pdf](https://github.com/hfp/libxsmm/raw/master/documentation/libxsmm.pdf)] [[src](https://github.com/hfp/libxsmm/archive/1.0.zip)] [![status](https://travis-ci.org/hfp/libxsmm.svg?branch=master \"Master branch build status\")](https://github.com/hfp/libxsmm/archive/master.zip)\r\n\r\n**What is a small matrix-matrix multiplication?** When characterizing the problem size using the M, N, and K parameters, a problem size suitable for LIBXSMM falls approximately within (M N K)^(1/3) \\<= 80 (which illustrates that non-square matrices or even \"tall and skinny\" shapes are covered as well). However the code generator only generates code up to the specified [threshold](#auto-dispatch). Raising the threshold may not only generate excessive amounts of code (due to unrolling), but also miss to implement a tiling scheme to effectively utilize the L2 cache. For problem sizes above the configurable threshold, LIBXSMM is falling back to BLAS.\r\n\r\n**How to determine whether an application can benefit from using LIBXSMM or not?** Given the application uses BLAS to carry out matrix multiplications, one may link against Intel MKL 11.2 (or higher), set the environment variable MKL_VERBOSE=1, and run the application using a representative workload (env MKL_VERBOSE=1 ./workload > verbose.txt). The collected output is the starting point for evaluating the problem sizes as imposed by the workload (grep -a \"MKL_VERBOSE DGEMM\" verbose.txt | cut -d, -f3-5).\r\n\r\n## Interface\r\nThe interface of the library is *generated* according to the [Build Instructions](#build-instructions), and is therefore **not** stored in the code repository. Instead, one may have a look at the code generation template files for [C/C++](https://github.com/hfp/libxsmm/blob/master/src/libxsmm.template.h) and [FORTRAN](https://github.com/hfp/libxsmm/blob/master/src/libxsmm.template.f90). To perform the matrix-matrix multiplication *c*<sub>*m* x *n*</sub> = *c*<sub>*m* x *n*</sub> + *a*<sub>*m* x *k*</sub> \\* *b*<sub>*k* x *n*</sub>, the following interfaces can be used:\r\n\r\n```C\r\n/** Initialization function to set up LIBXSMM's dispatching table. On may \r\n    call this routine to avoid lazy initialization overhead in the first \r\n    call to a LIBXSMM kernel routine */\r\nvoid libxsmm_build_static(); \r\n/** If non-zero function pointer is returned, call (*function)(M, N, K). */\r\nlibxsmm_smm_function libxsmm_smm_dispatch(int m, int n, int k);\r\nlibxsmm_dmm_function libxsmm_dmm_dispatch(int m, int n, int k);\r\n/** Automatically dispatched matrix-matrix multiplication. */\r\nvoid libxsmm_smm(int m, int n, int k, const float* a, const float* b, float* c);\r\nvoid libxsmm_dmm(int m, int n, int k, const double* a, const double* b, double* c);\r\n/** Non-dispatched matrix-matrix multiplication using inline code. */\r\nvoid libxsmm_simm(int m, int n, int k, const float* a, const float* b, float* c);\r\nvoid libxsmm_dimm(int m, int n, int k, const double* a, const double* b, double* c);\r\n/** Matrix-matrix multiplication using BLAS. */\r\nvoid libxsmm_sblasmm(int m, int n, int k, const float* a, const float* b, float* c);\r\nvoid libxsmm_dblasmm(int m, int n, int k, const double* a, const double* b, double* c);\r\n```\r\n\r\nWith C++ and FORTRAN function overloading, the library allows to omit the 's' and 'd' prefixes denoting the numeric type in the above C interface. Further, in C++ a type 'libxsmm_mm_dispatch<*type*>' can be used to instantiate a functor rather than making a distinction for the numeric type in 'libxsmm_?mm_dispatch'.\r\n\r\nNote: Function overloading in FORTRAN is just recommended when using automatically dispatched calls. When querying function pointers, please use the type \r\nspecific versions as the poly-morph version relies on C_LOC on arrays which GNU Fortran (gfortran) refuses to digest (as it is not specified in the FORTRAN standard).\r\n\r\n## Build Instructions\r\nTo generate the interface inside of the 'include' directory and to build the library, run one of the following commands (by default OFFLOAD=1 implies MIC=1):\r\n\r\n```\r\nmake\r\nmake MIC=1\r\nmake OFFLOAD=1\r\n```\r\n\r\nBy default, only the non-coprocessor target is built (OFFLOAD=0 and MIC=0). In general, the subfolders of the 'lib' directory are separating the build targets where the 'mic' folder is containing the native library (MIC=1) targeting the Intel Xeon Phi coprocessor (\"KNC\"), and the 'intel64' folder is storing either the hybrid archive made of CPU and coprocessor code (OFFLOAD=1), or an archive which is only containing the CPU code. By default, all libraries are built statically (STATIC=1).\r\n\r\nTo remove intermediate files (`make install` is a shortcut for `make; make clean`) or to remove all generated files and folders (including the interface and the library archives), run one of the following commands:\r\n\r\n```\r\nmake clean\r\nmake install\r\nmake realclean\r\n```\r\n\r\nThe library can be configured to accept row-major or column-major (default) order matrices. The row-major storage scheme is accomplished by setting ROW_MAJOR=1 (0 for column-major, and row-major otherwise):\r\n\r\n```\r\nmake ROW_MAJOR=1\r\n```\r\n\r\nBy default, LIBXSMM is not optimized for particular matrix sizes (M, N, and K values). Specializing the library for certain matrix sizes (and therefore optimizing the performance) can be achieved in the following way:\r\n\r\n```\r\nmake M=\"2 4\" N=\"1\" K=\"$(echo $(seq 2 5))\"\r\n```\r\n\r\nThe above example is generating the following set of (M,N,K) values:\r\n\r\n```\r\n(2,1,2), (2,1,3), (2,1,4), (2,1,5),\r\n(4,1,2), (4,1,3), (4,1,4), (4,1,5)\r\n```\r\n\r\nThe index sets are in a loop-nest relationship (M(N(K))) when generating the indices. Moreover, an empty index set resolves to the next non-empty outer index set of the loop nest (including to wrap around from the M to K set). An empty index set is not participating anymore in the loop-nest relationship. Here is an example of generating multiplication routines which are \"squares\" with respect to M and N (N inherits the current value of the \"M loop\"):\r\n\r\n```\r\nmake M=\"$(echo $(seq 2 5))\" K=\"$(echo $(seq 2 5))\"\r\n```\r\n\r\nAn even more flexible specialization is possible by using the MNK variable when building the library. It takes a list of indices which are eventually grouped (using commas):\r\n\r\n```\r\nmake MNK=\"2 3, 23\"\r\n```\r\n\r\nEach group of indexes is combined into all possible triplets generating the following set of (M,N,K) values:\r\n\r\n```\r\n(2,2,2), (2,2,3), (2,3,2), (2,3,3),\r\n(3,2,2), (3,2,3), (3,3,2), (3,3,3), (23,23,23)\r\n```\r\n\r\nTesting the generated cases means capturing the console output of the [cp2k](https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k.cpp) code sample:\r\n\r\n```\r\nmake MNK=\"2 3, 23\" test\r\n```\r\n\r\nThe recorded output file can be further evaluated. For example:\r\n\r\n```\r\ngrep \"diff\" samples/cp2k/cp2k-perf.txt | grep -v \"diff=0.000\"\r\n```\r\n\r\n## Performance\r\n### Tuning\r\nThe build system allows to conveniently select the target system using an AVX flag when invoking 'make'. The default is to generate code according to the feature bits of the host system running the compiler. The device-side defaults to \"MIC\" targeting the Intel Xeon Phi family of coprocessors (\"KNC\"). However beside of generating all supported host code paths (and letting the compiler pick the one representing the host), specifying a particular code path will not only save some time when generating the code (\"printing\"), but also enable cross-compilation for a target that is different from the compiler's host: SSE=3 (in fact SSE!=0), AVX=1, AVX=2 (with FMA), and AVX=3 are supported. The latter is targeting the Intel Knights Landing processor family (\"KNL\") and future Intel Xeon processors using Intel AVX-512 foundation instructions (AVX-512F):\r\n\r\n```\r\nmake AVX=3\r\n```\r\n\r\nThe library supports generating code using an \"implicitly aligned leading dimension\" for the destination matrix of a multiplication. The latter is enabling aligned store instructions, and also hints the inlinable C/C++ code accordingly. The client code may be arranged accordingly by checking the build parameters of the library (at compile-time using the preprocessor). Aligned store instructions imply a leading dimension which is a multiple of the default alignment:\r\n\r\n```\r\nmake ALIGNED_STORES=1\r\n```\r\n\r\nThe default alignment (ALIGNMENT=64) as well as a non-default alignment for the store instructions (ALIGNED_STORES=n) can be specified when invoking 'make'. The \"implicitly aligned leading dimension\" optimization is not expected to have a big impact due to the relatively low amount of store instructions in the instruction mix. In contrast, supporting an \"implicitly aligned leading dimension\" for loading the input matrices is supposed to make a bigger impact, however this is not yet exposed by the build system because: (1) aligning a batch of input matrices implies usually larger code changes for the client code whereas accumulating into a local temporary destination matrix is a relatively minor change, and (2) today's Advanced Vector Extensions including the AVX-512 capable hardware supports unaligned load/store instructions.\r\n\r\nThe generated interface of the library also encodes the parameters the library was built for (static information). This helps optimizing client code related to the library's functionality. For example, the LIBXSMM_MAX_* and LIBXSMM_AVG_* information can be used with the LIBXSMM_PRAGMA_LOOP_COUNT macro in order to hint loop trip counts when handling matrices related to LIBXSMM's problem domain.\r\n\r\n### Auto-dispatch\r\nThe function 'libxsmm_?mm_dispatch' helps amortizing the cost of the dispatch when multiple calls with the same M, N, and K are needed. In contrast, the automatic code dispatch is orchestrating three levels:\r\n\r\n1. Specialized routine (implemented in assembly code),\r\n2. Inlinable C/C++ code or optimized FORTRAN code, and\r\n3. BLAS library call.\r\n\r\nAll three levels are accessible directly (see [Interface](#interface)) allowing to customize the code dispatch. The level 2 and 3 may be supplied by the Intel Math Kernel Library (Intel MKL) 11.2 DIRECT CALL feature. Beside of the generic interface, one can also call a specific kernel e.g., 'libxsmm_dmm_4_4_4' multiplying 4x4 matrices. For the latter, the code generator generates prototypes for all specialized functions (interface).\r\n\r\nFurther, a preprocessor symbol denotes the largest problem size (*M* x *N* x *K*) that belongs to level (1) and (2), and therefore determines if a matrix multiplication falls back to level (3) of calling the LAPACK/BLAS library alongside of LIBXSMM. This threshold can be configured using for example:\r\n\r\n```\r\nmake THRESHOLD=$((60 * 60 * 60))\r\n```\r\n\r\nThe maximum of the given threshold and the largest requested specialization refines the value of the threshold. If a problem size falls below the threshold, dispatching the code requires to figure out whether a specialized routine exists or not.\r\n\r\n### JIT Backend\r\nThere might be situations in which it is up-front not clear which problem sizes will be needed during an application run. In order to leverage LIBXSMM's\r\nhigh-performance kernels here, LIBXSMM offers an experimental JIT (just-in-time) backend which generates the requested kernels on the fly. This is done by emitting the\r\ncorresponding byte-code directly into an executable buffer to ensure highest performance during the generation process. As the JIT backend is still experimental,\r\nsome limitations are in place:\r\n\r\n1. There is no support for SSE3 (Intel Xeon 5500/5600 series) and IMCI (Intel Xeon Phi coprocessor code-named Knights Corner) instruction set extensions\r\n1. LIBXSMM uses Pthread mutexes to guard updates of the JITted code cache, make sure to link with -lpthread; building with OMP=1 employs an OpenMP critical section as an alternative locking mechanism.\r\n\r\nThe JIT backend version of the LIBXSMM can be built by:\r\n\r\n```\r\nmake JIT=1\r\n```\r\n\r\nYou can use the aforementioned THRESHOLD parameter to control the matrix sizes for which the JIT compilation will be performed. However, explicitly requested kernels (by calling libxsmm_build_jit) are not subject to a problem size threshold. Moreover, building with JIT=2 (in fact, 1<JIT and JIT!=0) allows to solely rely on explicitly generating kernels at runtime.\r\n\r\nNotes: Modern Linux distributions have support for transparent huge pages (THP). LIBXSMM takes care of this feature when setting execute permissions of the code\r\ncache's pages. However, we measured up to 30 percent slowdown when running JITted code which was stored on THP. For systems with kernel 2.6.38 or later it is possible\r\nto disabled the usage of THP of mmap regions. Please modify [src/libxsmm_build.c](https://github.com/hfp/libxsmm/blob/master/src/libxsmm_build.c#L158), if you think you got hit by this problem (you need to remove the \"-c89\" flag when building LIBXSMM):\r\n\r\n```C\r\nint l_fd = open(\"/dev/zero\", O_RDWR);\r\nvoid* p = mmap(0, l_code_page_size, PROT_READ|PROT_WRITE, MAP_PRIVATE, l_fd, 0);\r\nclose(l_fd);\r\n/* explicitly disable THP for this memory region */ \r\nmadvise(p, l_code_page_size, MADV_NOHUGEPAGE);\r\n```\r\n\r\n### Directly invoking the generator backend\r\nIn some special, extremely performance-critical situations it might be useful to bypass LIBXSMM's frontend entirely and to directly call into the generated code. This is possible by either invoking the code generator after a regular build process (as described above) or by just building the backend and invoking the generator:\r\n\r\n```\r\nmake generator\r\nbin/generator\r\n```\r\n\r\nThe generator application requires following inputs:\r\n\r\n1. dense/dense_asm/sparse (dense create C file, dense_asm creates ASM)\r\n2. Filename to append\r\n3. Routine name to be created in 2.\r\n4. M parameter\r\n5. N parameter\r\n6. K parameter\r\n7. LDA (0 when 1. is \"sparse\" indicates A is sparse)\r\n8. LDB (0 when 1. is \"sparse\" indicates B is sparse)\r\n9. LDC parameter\r\n10. alpha (currently only 1)\r\n11. beta (0 or 1)\r\n12. Alignment override for A (1 auto, 0 no alignment)\r\n13. Alignment override for C ( 1 auto, 0 no alignment)\r\n14. Prefetching mode (just dense & dense_asm, see next list)\r\n15. SP/DP single or double precision\r\n16. CSC file (just required when 1. is \"sparse\"). Matrix market format.\r\n\r\nThe prefetch specifier can be:\r\n\r\n1. \"nopf\": no prefetching at all, just 3 inputs (\\*A, \\*B, \\*C)\r\n2. \"pfsigonly\": just prefetching signature, 6 inputs (\\*A, \\*B, \\*C, \\*A’, \\*B’, \\*C’)\r\n3. \"BL2viaC\": uses accesses to \\*C to prefetch \\*B’\r\n4. \"AL2\": uses accesses to \\*A to prefetch \\*A’\r\n5. \"curAL2\": prefetches current \\*A ahead in the kernel\r\n6. \"AL2_BL2viaC\": combines AL2 and BL2viaC\r\n7. \"curAL2_BL2viaC\": combines curAL2 and BL2viaC\r\n8. \"AL2jpst\": aggressive \\*A’ prefetch of first rows without any structure\r\n9. \"AL2jpst_BL2viaC\": combines AL2jpst and BL2viaC\r\n\r\nSince this is a complicated, here come some examples:\r\n\r\n```\r\nbin/generator dense foo.c foo 16 16 16 32 32 32 1 1 1 1 hsw nopf DP\r\nbin/generator dense_asm foo.c foo 16 16 16 32 32 32 1 1 1 1 knl AL2_BL2viaC DP\r\nbin/generator sparse foo.c foo 16 16 16 32 0 32 1 1 1 1 hsw nopf DP bar.csc\r\n```\r\n\r\nPlease note, in samples/generator and samples/seissol examples are shown that directly use LIBXSMM's generator backend.\r\n\r\n### Results\r\nThe library does not claim to be \"optimal\" or \"best-performing\", and the presented results are modeling certain applications which are not representative \"in general\". Instead, information on how to reproduce the results are given for each of the reported cases.\r\n\r\n![This performance plot for a dual-socket Intel Xeon E5-2699v3 (\"Haswell\") shows a \"compact selection\" (to make the plot visually more appealing) out of 386 specializations as useful for CP2K Open Source Molecular Dynamics [1]. The code has been generated and built by running \"./[make.sh](https://github.com/hfp/libxsmm/blob/master/make.sh) -cp2k AVX=2 -j\" (for Version 1.0). This and below plots were generated by running \"cd samples/cp2k ; ./[cp2k-plot.sh](https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-plot.sh) specialized cp2k-specialized.png -1\". Please note that larger problem sizes (MNK) carry a higher arithmetic intensity which usually leads to higher performance (less bottlenecked by memory bandwidth).](https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/1-cp2k-specialized.png)\r\n> This performance plot for a dual-socket Intel Xeon E5-2699v3 (\"Haswell\") shows a \"compact selection\" (to make the plot visually more appealing) out of 386 specializations as useful for CP2K Open Source Molecular Dynamics [1]. The code has been generated and built by running \"./[make.sh](https://github.com/hfp/libxsmm/blob/master/make.sh) -cp2k AVX=2 -j\" (with release 1.0). This and below plots were generated by running \"cd samples/cp2k ; ./[cp2k-plot.sh](https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-plot.sh) specialized cp2k-specialized.png -1\". Please note that larger problem sizes (MNK) carry a higher arithmetic intensity which usually leads to higher performance (less bottlenecked by memory bandwidth).\r\n\r\n![This plot summarizes the performance of the generated kernels by averaging the results over K (and therefore the bar on the right hand side may not show the same maximum when compared to other plots). The performance is well-tuned across the parameter space with no \"cold islands\", and the lower left \"cold\" corner is fairly limited. Please refer to the first figure on how to reproduce the results.](https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/2-cp2k-specialized.png)\r\n> This plot summarizes the performance of the generated kernels by averaging the results over K (and therefore the bar on the right hand side may not show the same maximum when compared to other plots). The performance is well-tuned across the parameter space with no \"cold islands\", and the lower left \"cold\" corner is fairly limited. Please refer to the first figure on how to reproduce the results.\r\n\r\n![This plot shows the arithmetic average (non-sliding) of the performance with respect to groups of problem sizes (MNK). The problem sizes are binned into three groups according to the shown intervals: \"Small\", \"Medium\", and \"Larger\" (notice that \"larger\" may still not be a large problem size). Please refer to the first figure on how to reproduce the results.](https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/3-cp2k-specialized.png)\r\n> This plot shows the arithmetic average (non-sliding) of the performance with respect to groups of problem sizes (MNK). The problem sizes are binned into three groups according to the shown intervals: \"Small\", \"Medium\", and \"Larger\" (notice that \"larger\" may still not be a large problem size). Please refer to the first figure on how to reproduce the results.\r\n\r\n![In order to further summarize the previous plots, this graph shows the cumulative distribution function (CDF) of the performance across all cases. Similar to the median value at 50%, one can read for example that 100% of the cases are yielding less or equal the largest discovered value. The value highlighted by the arrows is usually the median value, the [plot script](https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-perf.plt) however attempts to highlight a single \"fair performance value\" representing all cases by linearly fitting the CDF, projecting onto the x-axis, and taking the midpoint of the projection (usually at 50%). Please note, that this diagram shows a statistical distribution and does not allow to identify any particular kernel. Moreover at any point of the x-axis (\"Probability\"), the \"Compute Performance\" and the \"Memory Bandwidth\" graph do not necessarily belong to the same kernel! Please refer to the first figure on how to reproduce the results.](https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/4-cp2k-specialized.png)\r\n> In order to further summarize the previous plots, this graph shows the cumulative distribution function (CDF) of the performance across all cases. Similar to the median value at 50%, one can read for example that 100% of the cases are yielding less or equal the largest discovered value. The value highlighted by the arrows is usually the median value, the [plot script](https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-perf.plt) however attempts to highlight a single \"fair performance value\" representing all cases by linearly fitting the CDF, projecting onto the x-axis, and taking the midpoint of the projection (usually at 50%). Please note, that this diagram shows a statistical distribution and does not allow to identify any particular kernel. Moreover at any point of the x-axis (\"Probability\"), the \"Compute Performance\" and the \"Memory Bandwidth\" graph do not necessarily belong to the same kernel! Please refer to the first figure on how to reproduce the results.\r\n\r\n## Implementation\r\n### Limitations\r\nBeside of the inlinable C or optimized FORTRAN code path, the library is currently limited to a single code path which is selected at build time of the library. Without a specific flag (SSE=1, AVX=1|2|3), the assembly code generator emits code for all supported instruction set extensions. However, the compiler is picking only one of the generated code paths according to its code generation flags (or according to what is native with respect to the compiler-host). A future version of the library may be including all code paths at build time and allow for runtime-dynamic dispatch of the most suitable code path.\r\n\r\nA future version of the library may support an auto-tuning stage when generating the code (to find M,N,K-combinations for specialized routines which are beneficial compared to the code generated from the inlinable C or optimized FORTRAN code path). Auto-tuning the compiler code generation using a profile-guided optimization may be another option to be incorporated into the build system (Makefile). However, auto-tuning also imposes a reasonable burden (similar to profile guided optimization) and the current library is supposed to generate extremely efficient code for everything that does not rely on L2 cache blocking (below THRESHOLD), and except for very small problem sizes where a dedicated kernel may beat our implementation (e.g., MNK=4x4x4).\r\n\r\n### Roadmap\r\nAlthough the library is under development, the published interface is rather stable and may only be extended in future revisions. The following issues are being addressed in upcoming revisions:\r\n\r\n* Full xGEMM interface, and extended code dispatcher\r\n* API supporting sparse matrices and other cases\r\n\r\n## Applications and References\r\n**\\[1] [http://cp2k.org/](http://cp2k.org/)**: Open Source Molecular Dynamics which (optionally) uses LIBXSMM. The application is generating batches of small matrix-matrix multiplications (\"matrix stack\") out of a problem-specific distributed block-sparse matrix (see https://github.com/hfp/libxsmm/raw/master/documentation/cp2k.pdf).\r\n\r\n**\\[2] [https://github.com/SeisSol/SeisSol/](https://github.com/SeisSol/SeisSol/)**: SeisSol is one of the leading codes for earthquake scenarios, in particular for simulating dynamic rupture processes. LIBXSMM provides highly optimized assembly kernels which form the computational back-bone of SeisSol (see https://github.com/TUM-I5/seissol_kernels/).\r\n\r\n**\\[3] [http://software.intel.com/xeonphicatalog](http://software.intel.com/xeonphicatalog)**: Intel Xeon Phi Applications and Solutions Catalog.\r\n\r\n**\\[4] [http://goo.gl/qsnOOf](https://software.intel.com/en-us/articles/intel-and-third-party-tools-and-libraries-available-with-support-for-intelr-xeon-phitm)**: Intel 3rd Party Tools and Libraries.\r\n","google":"UA-65670560-2","note":"Don't delete this file! It's used internally to help with page regeneration."}